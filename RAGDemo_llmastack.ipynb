{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecc1c704-43f5-41b9-819d-17ba43c3c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b3600a-e5b1-445a-97b1-84f8be173455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_stack\n",
      "  Downloading llama_stack-0.2.12-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (3.12.13)\n",
      "Collecting fastapi<1.0,>=0.115.0 (from llama_stack)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting fire (from llama_stack)\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: httpx in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (0.28.1)\n",
      "Collecting huggingface-hub (from llama_stack)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.6 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (4.24.0)\n",
      "Collecting llama-stack-client>=0.2.12 (from llama_stack)\n",
      "  Downloading llama_stack_client-0.2.12-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai>=1.66 (from llama_stack)\n",
      "  Downloading openai-1.106.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (3.0.51)\n",
      "Collecting python-dotenv (from llama_stack)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting python-jose (from llama_stack)\n",
      "  Downloading python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pydantic>=2 (from llama_stack)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (2.32.4)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (13.9.4)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (75.8.2)\n",
      "Collecting starlette (from llama_stack)\n",
      "  Downloading starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: termcolor in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (2.3.0)\n",
      "Collecting tiktoken (from llama_stack)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pillow in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (11.2.1)\n",
      "Requirement already satisfied: h11>=0.16.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack) (0.16.0)\n",
      "Collecting python-multipart>=0.0.20 (from llama_stack)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/app-root/lib64/python3.11/site-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/app-root/lib64/python3.11/site-packages (from fastapi<1.0,>=0.115.0->llama_stack) (4.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2>=3.1.6->llama_stack) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.12->llama_stack) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.12->llama_stack) (8.2.1)\n",
      "Collecting distro<2,>=1.7.0 (from llama-stack-client>=0.2.12->llama_stack)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.12->llama_stack) (2.2.3)\n",
      "Collecting pyaml (from llama-stack-client>=0.2.12->llama_stack)\n",
      "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.12->llama_stack) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.12->llama_stack) (4.67.1)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx->llama_stack) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx->llama_stack) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/app-root/lib64/python3.11/site-packages (from httpx->llama_stack) (3.10)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.66->llama_stack)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2->llama_stack)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2->llama_stack)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2->llama_stack)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->llama_stack) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->llama_stack) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->llama_stack) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->llama_stack) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->llama_stack) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->llama_stack) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->llama_stack) (1.20.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub->llama_stack) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub->llama_stack) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub->llama_stack) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub->llama_stack) (6.0.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub->llama_stack)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema->llama_stack) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema->llama_stack) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema->llama_stack) (0.25.1)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.11/site-packages (from prompt-toolkit->llama_stack) (0.2.13)\n",
      "Collecting ecdsa!=0.15 (from python-jose->llama_stack)\n",
      "  Downloading ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /opt/app-root/lib64/python3.11/site-packages (from python-jose->llama_stack) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.5.0 in /opt/app-root/lib64/python3.11/site-packages (from python-jose->llama_stack) (0.6.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->llama_stack) (1.26.20)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack) (2.19.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->llama_stack)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/app-root/lib64/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from ecdsa!=0.15->python-jose->llama_stack) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack) (0.1.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama-stack-client>=0.2.12->llama_stack) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama-stack-client>=0.2.12->llama_stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama-stack-client>=0.2.12->llama_stack) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama-stack-client>=0.2.12->llama_stack) (2025.2)\n",
      "Requirement already satisfied: pycparser in /opt/app-root/lib64/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Downloading llama_stack-0.2.12-py3-none-any.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m191.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading llama_stack_client-0.2.12-py3-none-any.whl (340 kB)\n",
      "Downloading openai-1.106.1-py3-none-any.whl (930 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m599.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m673.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m267.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m621.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m611.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m275.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m592.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-inspection, regex, python-multipart, python-dotenv, pypdfium2, pydantic-core, pyaml, jiter, hf-xet, fire, ecdsa, distro, annotated-types, tiktoken, starlette, python-jose, pydantic, huggingface-hub, pdfminer.six, openai, llama-stack-client, fastapi, pdfplumber, llama_stack\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.22\n",
      "    Uninstalling pydantic-1.10.22:\n",
      "      Successfully uninstalled pydantic-1.10.22\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codeflare-sdk 0.29.0 requires pydantic<2, but you have pydantic 2.11.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 distro-1.9.0 ecdsa-0.19.1 fastapi-0.116.1 fire-0.7.1 hf-xet-1.1.9 huggingface-hub-0.34.4 jiter-0.10.0 llama-stack-client-0.2.12 llama_stack-0.2.12 openai-1.106.1 pdfminer.six-20250506 pdfplumber-0.11.7 pyaml-25.7.0 pydantic-2.11.7 pydantic-core-2.33.2 pypdfium2-4.30.0 python-dotenv-1.1.1 python-jose-3.5.0 python-multipart-0.0.20 regex-2025.9.1 starlette-0.47.3 tiktoken-0.11.0 typing-inspection-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama_stack pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d65686-cbf4-489d-a1a9-83b47604a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import RAGDocument, LlamaStackClient\n",
    "# Cell 2: extract text from website via SmolDocling (with BeautifulSoup fallback), save to `raw_text`\n",
    "import time\n",
    "from bs4 import BeautifulSoup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5464c819-8a01-4779-ad49-06adc4a8cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_endpoint= \"http://lsd-llama-milvus-service:8321\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2b1abf-a509-438d-8164-e781d760edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(identifier='granite', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='granite', model_type='llm'), Model(identifier='granite-embedding-125m', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')]\n"
     ]
    }
   ],
   "source": [
    "client = LlamaStackClient(base_url=deployment_endpoint)\n",
    "models= client.models.list()\n",
    "print(client.models.list())\n",
    "\n",
    "model_id = next(m.identifier for m in models if m.model_type == \"llm\")\n",
    "embedding_model = next(m for m in models if m.model_type == \"embedding\")\n",
    "embedding_model_id = embedding_model.identifier\n",
    "embedding_dimension = embedding_model.metadata[\"embedding_dimension\"]\n",
    "\n",
    "vector_db_id = \"my_milvus_db\"\n",
    "provider_id  = \"milvus\"\n",
    "\n",
    "# ### Do this step only once \n",
    "\n",
    "\n",
    "# _ = client.vector_dbs.register(\n",
    "# vector_db_id=vector_db_id,\n",
    "# embedding_model=embedding_model_id,\n",
    "# embedding_dimension=embedding_dimension,\n",
    "# provider_id=provider_id,\n",
    "# )\n",
    "# print(f\"Registered vector DB: {vector_db_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff345d59-cb61-4264-b6fb-0a28e215e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provider_id = \"sentence-transformers\"   # from your providers output\n",
    "# embedding_model_local_id = \"nomic-embed-text-v1.5-768\"   # local id you choose\n",
    "# provider_model_id = \"nomic-ai/nomic-embed-text-v1.5\"     # HF repo id\n",
    "# embedding_dimension = 768   # choose 768, 512, 256, 128, or 64 (nomic supports variable dims)\n",
    "\n",
    "# try:\n",
    "#     resp = client.models.register(\n",
    "#         model_id=embedding_model_local_id,\n",
    "#         provider_id=provider_id,\n",
    "#         provider_model_id=provider_model_id,\n",
    "#         model_type=\"embedding\",\n",
    "#         metadata={\"description\": \"Nomic embed text v1.5 (768-d)\", \"embedding_dimension\": float(embedding_dimension)}\n",
    "#     )\n",
    "#     print(\"Registered embedding model:\", embedding_model_local_id, resp)\n",
    "# except Exception as e:\n",
    "#     print(\"Register embedding model error:\", e)\n",
    "\n",
    "# # verify it appears\n",
    "# print(\"models after register:\", [(m.identifier, m.model_type, getattr(m, \"metadata\", None)) for m in client.models.list()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9f8c829-de1a-4521-820a-814a19da18ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P28' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolDocling endpoints failed (parse/predict). Falling back to local extraction.\n",
      "Extracted text using pdfplumber fallback.\n",
      "Saved extracted text to raw_text.txt; raw_text length = 70894 characters.\n",
      "--- preview (first 800 chars) ---\n",
      "[page:1]\n",
      "State Bank of India\n",
      "Central Recruitment & Promotion Department\n",
      "Corporate Centre, Mumbai\n",
      "Phone: 022-22820427; e-mail: crpd@sbi.co.in\n",
      "SBI HONOURED AS OVERALL WINNER UNDER “TOP PERFORMING BANK”\n",
      "CATEGORY AT EA SE 7.0 CITATION CEREMONY\n",
      "Page 1 of 11\n",
      "\n",
      "[page:2]\n",
      "State Bank of India\n",
      "CENTRAL RECRUITMENT & PROMOTION DEPARTMENT\n",
      "CORPORATE CENTRE, MUMBAI\n",
      "(Phone: 022-2282 0427; E-mail: crpd@sbi.co.in)\n",
      "RECRUITMENT OF JUNIOR ASSOCIATES (CUSTOMER SUPPORT & SALES)\n",
      "(Advertisement No. CRPD/CR/2025-26/06)\n",
      "ONLINE REGISTRATION OF APPLICATION AND PAYMENT OF FEES: 06.08.2025 TO 26.08.2025\n",
      "Applications are invited from eligible Indian Citizens for appointment as Junior Associate (Customer Support & Sales) in clerical cadre in State Bank of India. Candidates\n",
      "can apply for vacancies in one State/UT only. Candi\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: extract text via SmolDocling (with pdfplumber fallback), save to `raw_text`\n",
    "def extract_text_with_smol_and_save(pdf_path: str, smol_endpoint: str = SMOLDOCLING_ENDPOINT, out_txt: str = \"raw_text.txt\") -> str:\n",
    "    \"\"\"\n",
    "    1) Try to POST the PDF to SmolDocling's parse endpoint (assumes POST <endpoint>/parse_pdf accepts multipart/form-data file).\n",
    "    2) If that fails or returns no text, fallback to local extraction using pdfplumber.\n",
    "    3) Save the final text to out_txt and also return it (and set raw_text).\n",
    "    \"\"\"\n",
    "    # read bytes\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "\n",
    "    # 1) Try SmolDocling parse route\n",
    "    extracted = \"\"\n",
    "    try:\n",
    "        files = {\"file\": (pdf_path, pdf_bytes, \"application/pdf\")}\n",
    "        # attempt parse route first\n",
    "        resp = requests.post(smol_endpoint.rstrip(\"/\") + \"/parse_pdf\", files=files, timeout=300)\n",
    "        resp.raise_for_status()\n",
    "        j = resp.json()\n",
    "        extracted = j.get(\"text\") or j.get(\"content\") or \"\"\n",
    "        if extracted and extracted.strip():\n",
    "            print(\"Extracted text from SmolDocling /parse_pdf\")\n",
    "    except Exception as e_parse:\n",
    "        # try a generic predict route if parse_pdf doesn't exist\n",
    "        try:\n",
    "            payload = {\"file_name\": pdf_path}\n",
    "            # some deployments might accept multipart on /predict or different shapes - try /predict with same files\n",
    "            resp2 = requests.post(smol_endpoint.rstrip(\"/\") + \"/predict\", files=files, timeout=300)\n",
    "            resp2.raise_for_status()\n",
    "            j2 = resp2.json()\n",
    "            extracted = j2.get(\"text\") or j2.get(\"content\") or j2.get(\"generated_text\") or \"\"\n",
    "            if extracted and extracted.strip():\n",
    "                print(\"Extracted text from SmolDocling /predict\")\n",
    "        except Exception as e_predict:\n",
    "            print(\"SmolDocling endpoints failed (parse/predict). Falling back to local extraction.\")\n",
    "            # will fallback below\n",
    "\n",
    "    # 2) Fallback to pdfplumber if Smol didn't return text\n",
    "    if not extracted or not extracted.strip():\n",
    "        try:\n",
    "            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "                pages = []\n",
    "                for i, p in enumerate(pdf.pages):\n",
    "                    pages.append(f\"[page:{i+1}]\\n\" + (p.extract_text() or \"\"))\n",
    "            extracted = \"\\n\\n\".join(pages)\n",
    "            print(\"Extracted text using pdfplumber fallback.\")\n",
    "        except Exception as e_local:\n",
    "            # if even fallback fails, keep an empty string and raise/log\n",
    "            print(\"Local pdfplumber extraction also failed:\", e_local)\n",
    "            extracted = \"\"\n",
    "\n",
    "    # 3) Save to file and return\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(extracted or \"\")\n",
    "\n",
    "    # put into a global variable raw_text for convenience in notebook\n",
    "    global raw_text\n",
    "    raw_text = extracted or \"\"\n",
    "    print(f\"Saved extracted text to {out_txt}; raw_text length = {len(raw_text)} characters.\")\n",
    "    return raw_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98c376-803b-42d6-9702-98ff9082632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute extraction (will set raw_text)\n",
    "raw_text = extract_text_with_smol_and_save(pdf_path)\n",
    "# quick preview\n",
    "print(\"--- preview (first 800 chars) ---\")\n",
    "print(raw_text[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a473ab71-603a-44e5-8306-132f3e872708",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = RAGDocument(\n",
    "document_id=\"raw_text_001\",\n",
    "content=raw_text,\n",
    "mime_type=\"text/plain\",\n",
    "metadata={\"source\": \"SBI_Doc\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dfc2ce3-5c74-4c7c-99c0-99feee20bcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text ingested successfully\n"
     ]
    }
   ],
   "source": [
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=[document],\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=100,\n",
    ")\n",
    "print(\"Raw text ingested successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea9372c0-9005-4cd7-aeac-1da95fddedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example RAG query for one-off lookups\n",
    "query = \"What is condition for uploading photographs for SBI RECRUITMENT OF JUNIOR ASSOCIATES (CUSTOMER SUPPORT & SALES 2025 ?\"\n",
    "\n",
    "\n",
    "ground_truth =\"\"\" Photograph Image: (4.5 cm x 3.5 cm)\n",
    "• Photograph must be a recent passport style colour picture.\n",
    "• Make sure that the picture is in colour, taken against a light-coloured, preferably white,\n",
    "background.\n",
    "• Look straight at the camera with a relaxed face\n",
    "• If picture is taken on a sunny day, have the sun behind you, or place yourself in shade,\n",
    "so that you are not squinting and there are no harsh shadows\n",
    "• If you have to use flash, ensure there's no \"red-eye\"\n",
    "• If you wear glasses make sure that there are no reflections and your eyes can be\n",
    "clearly seen.\n",
    "• Caps, hats and dark glasses are not acceptable. Religious headwear is allowed but it\n",
    "must not cover your face.\n",
    "• Dimensions 200 x 230 pixels (preferred)\n",
    "• Size of file should be between 20 kb–50 kb\n",
    "• Ensure that size of the scanned image is not more than 50kb. If the size of the file is\n",
    "more than 50 kb, then adjust the settings of the scanner such as the DPI resolution,\n",
    "no. of colours etc., during the process of scanning.\n",
    "• Photo uploaded should be of appropriate size and clearly visible.\n",
    "• It is advisable that candidate retains about 8 copies of the same photograph\n",
    "which is uploaded at the time of online application as these would be needed for\n",
    "further processes of this selection process.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa939aa-5f76-4799-963f-e79637a153f1",
   "metadata": {},
   "source": [
    "## LLM Response (With RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "274cbbfe-5d5c-4fbf-beb9-b906ff97b316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the SBI RECRUITMENT OF JUNIOR ASSOCIATES (CUSTOMER SUPPORT & SALES 2025, the conditions for uploading photographs are as follows:\n",
      "\n",
      "1. The photograph must be of a candidate's face, clearly visible with blue ink.\n",
      "2. The photo should be in jpg/jpeg format and have dimensions of 240 x 240 pixels in 200 DPI (preferred for required quality, i.e., 3 cm x 3 cm).\n",
      "3. If the photo is not uploaded at the specified place, admission for the examination will be rejected/denied.\n",
      "4. The photo should be captured against a light, preferably coloured, and clicked.\n",
      "5. The photo should not be a small size.\n",
      "6. The photo should be of appropriate size and clearly visible.\n",
      "7. Candidates are advised to retain about 8 copies of the same photograph.\n",
      "8. The photograph will get auto-uploaded in the application form.\n",
      "9. If the face in the photograph, signature, left thumb impression, or handwritten declaration is unclear or smudged, the candidate's application may be rejected.\n",
      "\n",
      "The photograph must be uploaded during the process of filling in the Online Application Form, and candidates will be provided with links to upload their photograph, signature, left thumb impression, and handwritten declaration. The total document size should not exceed 50 kb. If it does, candidates should adjust the scanner settings such as DPI resolution, number of colours, etc., during the scanning process.\n"
     ]
    }
   ],
   "source": [
    "# Query chunks\n",
    "rag_result = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=query,\n",
    ")\n",
    "\n",
    "# Build context\n",
    "context = \"\\n\\n\".join([item.text for item in rag_result.content if hasattr(item, \"text\")])\n",
    "\n",
    "# Ask the model\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Answer using only the provided CONTEXT.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\"}\n",
    "]\n",
    "\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "print(resp.completion_message.content)\n",
    "rag_answer= resp.completion_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4eec36-1388-4c07-9111-42271f83048f",
   "metadata": {},
   "source": [
    "## LLM response (Without RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eeca8283-ed8c-4985-b7d1-bc84e65f001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base LLM Answer:\n",
      " For SBI Recruitment of Junior Associates (Customer Support & Sales 2025), photographs are not explicitly mentioned as a requirement in the official notification. However, it's advisable to check the latest updates on the State Bank of India (SBI) recruitment website or contact the recruitment department for the most accurate and current information regarding photograph upload conditions.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\\nQUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "]\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "# Extract final answer text\n",
    "answer = None\n",
    "if hasattr(resp, \"completion_message\") and resp.completion_message:\n",
    "    answer = resp.completion_message.content\n",
    "elif hasattr(resp, \"choices\") and resp.choices:\n",
    "    answer = resp.choices[0].message.content\n",
    "else:\n",
    "    answer = str(resp)\n",
    "\n",
    "print(\"Base LLM Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18d765-d349-4163-9747-89351b1d4aa7",
   "metadata": {},
   "source": [
    "## Comparison between actual answer , RAG & Without RAG Generated Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9753d569-8f5c-4526-ae52-423d29291984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ground Truth ===\n",
      "\n",
      " Photograph Image: (4.5 cm x 3.5 cm)\n",
      "• Photograph must be a recent passport style colour picture.\n",
      "• Make sure that the picture is in colour, taken against a light-coloured, preferably white,\n",
      "background.\n",
      "• Look straight at the camera with a relaxed face\n",
      "• If picture is taken on a sunny day, have the sun behind you, or place yourself in shade,\n",
      "so that you are not squinting and there are no harsh shadows\n",
      "• If you have to use flash, ensure there's no \"red-eye\"\n",
      "• If you wear glasses make sure that there are no reflections and your eyes can be\n",
      "clearly seen.\n",
      "• Caps, hats and dark glasses are not acceptable. Religious headwear is allowed but it\n",
      "must not cover your face.\n",
      "• Dimensions 200 x 230 pixels (preferred)\n",
      "• Size of file should be between 20 kb–50 kb\n",
      "• Ensure that size of the scanned image is not more than 50kb. If the size of the file is\n",
      "more than 50 kb, then adjust the settings of the scanner such as the DPI resolution,\n",
      "no. of colours etc., during the process of scanning.\n",
      "• Photo uploaded should be of appropriate size and clearly visible.\n",
      "• It is advisable that candidate retains about 8 copies of the same photograph\n",
      "which is uploaded at the time of online application as these would be needed for\n",
      "further processes of this selection process.\n",
      "\n",
      "=== RAG Generated Answer ===\n",
      "\n",
      "For the SBI RECRUITMENT OF JUNIOR ASSOCIATES (CUSTOMER SUPPORT & SALES 2025, the conditions for uploading photographs are as follows:\n",
      "\n",
      "1. The photograph must be of a candidate's face, clearly visible with blue ink.\n",
      "2. The photo should be in jpg/jpeg format and have dimensions of 240 x 240 pixels in 200 DPI (preferred for required quality, i.e., 3 cm x 3 cm).\n",
      "3. If the photo is not uploaded at the specified place, admission for the examination will be rejected/denied.\n",
      "4. The photo should be captured against a light, preferably coloured, and clicked.\n",
      "5. The photo should not be a small size.\n",
      "6. The photo should be of appropriate size and clearly visible.\n",
      "7. Candidates are advised to retain about 8 copies of the same photograph.\n",
      "8. The photograph will get auto-uploaded in the application form.\n",
      "9. If the face in the photograph, signature, left thumb impression, or handwritten declaration is unclear or smudged, the candidate's application may be rejected.\n",
      "\n",
      "The photograph must be uploaded during the process of filling in the Online Application Form, and candidates will be provided with links to upload their photograph, signature, left thumb impression, and handwritten declaration. The total document size should not exceed 50 kb. If it does, candidates should adjust the scanner settings such as DPI resolution, number of colours, etc., during the scanning process.\n",
      "\n",
      "=== Base LLM Answer (without RAG) ===\n",
      "\n",
      "For SBI Recruitment of Junior Associates (Customer Support & Sales 2025), photographs are not explicitly mentioned as a requirement in the official notification. However, it's advisable to check the latest updates on the State Bank of India (SBI) recruitment website or contact the recruitment department for the most accurate and current information regarding photograph upload conditions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ground Truth ===\\n\")\n",
    "print(ground_truth)\n",
    "\n",
    "print(\"\\n=== RAG Generated Answer ===\\n\")\n",
    "print(rag_answer)\n",
    "\n",
    "print(\"\\n=== Base LLM Answer (without RAG) ===\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73694ebf-4196-415a-846a-5d4d31af7e3e",
   "metadata": {},
   "source": [
    "### RAG Agent - This is advanced RAG feature present in Llamastack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ee2c3d0-e663-4ff0-bb21-c2fc4c75a0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SBI's Recruitment of Junior Associates (Customer Support & Sales 2025), the conditions for uploading photographs are:\n",
      "\n",
      "1. The photo must be a clear, colored image of the candidate's face, taken against a light.\n",
      "2. The photo should be of appropriate size, ideally 240 x 240 pixels in 200 DPI (3 cm x 3 cm).\n",
      "3. The file type should be jpg/jpeg.\n",
      "4. If the photo is not uploaded at the specified place, admission for the examination will be rejected.\n",
      "5. The photograph will be auto-uploaded in the application form.\n",
      "6. If the face in the photograph is unclear or smudged, the candidate's application may be rejected.\n",
      "7. Candidates are advised to retain 8 copies of the same photograph.\n"
     ]
    }
   ],
   "source": [
    "# 1) Run retrieval explicitly (same tool used by the agent)\n",
    "rag_result = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=query,\n",
    "    query_config={\n",
    "        \"chunk_size_in_tokens\": 512,\n",
    "        \"chunk_overlap_in_tokens\": 100,\n",
    "        \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2) Extract the retrieved text chunks (simple join)\n",
    "retrieved_texts = []\n",
    "for item in getattr(rag_result, \"content\", []) or []:\n",
    "    # item may be dict-like or object; handle both\n",
    "    txt = item.get(\"text\") if isinstance(item, dict) else getattr(item, \"text\", None)\n",
    "    if not txt:\n",
    "        continue\n",
    "    # remove our template labels to keep only chunk text\n",
    "    import re\n",
    "    cleaned = re.sub(r\"^Result\\s*\\d+\\s*\", \"\", txt, flags=re.I)\n",
    "    cleaned = re.sub(r\"Content:\\s*\", \"\", cleaned, flags=re.I)\n",
    "    cleaned = re.sub(r\"\\nMetadata:.*$\", \"\", cleaned, flags=re.S|re.I)\n",
    "    cleaned = cleaned.strip()\n",
    "    if cleaned:\n",
    "        retrieved_texts.append(cleaned)\n",
    "\n",
    "context = \"\\n\\n\".join(retrieved_texts).strip()\n",
    "\n",
    "# 3) Pick an LLM model (first available LLM)\n",
    "models = client.models.list()\n",
    "model_id = next((m.identifier for m in models if getattr(m, \"model_type\", None) == \"llm\"), None)\n",
    "if model_id is None:\n",
    "    raise RuntimeError(\"No LLM model found in client.models.list()\")\n",
    "\n",
    "# 4) Call the inference API (chat completion) with the retrieved context + user question\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. USE ONLY the provided CONTEXT to answer.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "]\n",
    "\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "# 5) Extract final answer text from the inference response and print it\n",
    "final_text = None\n",
    "if hasattr(resp, \"completion_message\") and getattr(resp, \"completion_message\") is not None:\n",
    "    final_text = getattr(resp.completion_message, \"content\", None) or getattr(resp.completion_message, \"text\", None)\n",
    "if not final_text and hasattr(resp, \"choices\") and resp.choices:\n",
    "    c0 = resp.choices[0]\n",
    "    if hasattr(c0, \"message\"):\n",
    "        final_text = getattr(c0.message, \"content\", None) or getattr(c0.message, \"text\", None)\n",
    "if not final_text:\n",
    "    # fallback to string representation\n",
    "    final_text = str(resp)\n",
    "\n",
    "print(final_text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe68392-abc3-4138-b8b8-d5a829abdbb0",
   "metadata": {},
   "source": [
    "### URL RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def9543c-ac42-4a4f-b5c4-4cf59efd34cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolDocling endpoints failed for URL (parse/predict). Falling back to BeautifulSoup.\n",
      "Extracted text using BeautifulSoup fallback.\n",
      "Saved extracted text to raw_text.txt; raw_text length = 28473 characters.\n",
      "--- preview (first 800 chars) ---\n",
      "ï»¿\n",
      "\n",
      "YONO LITE SBI\n",
      "\n",
      "YONO LITE SBI - Frequently Asked Questions\n",
      "\n",
      "FAQ Features\n",
      "\n",
      "ADD AND MANAGE BENEFICIARY\n",
      "\n",
      "1. How do I add SBI beneficiary?\n",
      "\n",
      "Please follow the below steps:\n",
      "\n",
      "Go to 'Settings' >> 'Profile Management' >> 'Add/Manage Beneficiary'.\n",
      "\n",
      "Give your profile password and click on 'Submit' button.\n",
      "\n",
      "Click on 'Add' icon on the right hand corner.\n",
      "\n",
      "Select 'State Bank Account' from dropdown.\n",
      "\n",
      "Provide the account number and limit and click on 'Submit' button.\n",
      "\n",
      "Confirm the details in the pre confirm screen.\n",
      "\n",
      "Click on 'Submit' button.\n",
      "\n",
      "Provide the OTP received in mobile and click on 'Submit' button.\n",
      "\n",
      "2. How do I add other bank beneficiary?\n",
      "\n",
      "Please follow the below steps:\n",
      "\n",
      "Go to 'Settings' >> 'Profile Management' >> 'Add/Manage Beneficiary'.\n",
      "\n",
      "Provide your profile password and click on 'Submit' but\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_text_from_url_with_smol_and_save(\n",
    "    url: str,\n",
    "    smol_endpoint: str = SMOLDOCLING_ENDPOINT,\n",
    "    out_txt: str = \"raw_text.txt\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    1) Try to POST the URL to SmolDocling's parse endpoint (assumes POST <endpoint>/parse accepts JSON {\"url\": <url>}).\n",
    "    2) If that fails or returns no text, fallback to local extraction using requests + BeautifulSoup.\n",
    "    3) Save the final text to out_txt and also return it (and set raw_text).\n",
    "    \"\"\"\n",
    "    extracted = \"\"\n",
    "\n",
    "    # 1) Try SmolDocling /parse endpoint for URL\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            smol_endpoint.rstrip(\"/\") + \"/parse\",\n",
    "            json={\"url\": url},\n",
    "            timeout=300,\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        j = resp.json()\n",
    "        extracted = j.get(\"text\") or j.get(\"content\") or j.get(\"extracted_text\") or \"\"\n",
    "        if extracted and extracted.strip():\n",
    "            print(\"Extracted text from SmolDocling /parse (URL)\")\n",
    "    except Exception as e_parse:\n",
    "        try:\n",
    "            # try /predict endpoint if /parse is not available\n",
    "            resp2 = requests.post(\n",
    "                smol_endpoint.rstrip(\"/\") + \"/predict\",\n",
    "                json={\"url\": url},\n",
    "                timeout=300,\n",
    "            )\n",
    "            resp2.raise_for_status()\n",
    "            j2 = resp2.json()\n",
    "            extracted = (\n",
    "                j2.get(\"text\") or j2.get(\"content\") or j2.get(\"generated_text\") or \"\"\n",
    "            )\n",
    "            if extracted and extracted.strip():\n",
    "                print(\"Extracted text from SmolDocling /predict (URL)\")\n",
    "        except Exception as e_predict:\n",
    "            print(\"SmolDocling endpoints failed for URL (parse/predict). Falling back to BeautifulSoup.\")\n",
    "\n",
    "    # 2) Fallback to BeautifulSoup if Smol didn't return text\n",
    "    if not extracted or not extracted.strip():\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            for s in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\"]):\n",
    "                s.decompose()\n",
    "            lines = [ln.strip() for ln in soup.get_text(separator=\"\\n\").splitlines() if ln.strip()]\n",
    "            extracted = \"\\n\\n\".join(lines)\n",
    "            print(\"Extracted text using BeautifulSoup fallback.\")\n",
    "        except Exception as e_local:\n",
    "            print(\"Local BeautifulSoup extraction also failed:\", e_local)\n",
    "            extracted = \"\"\n",
    "\n",
    "    # 3) Save to file and return\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(extracted or \"\")\n",
    "\n",
    "    global raw_text\n",
    "    raw_text = extracted or \"\"\n",
    "    print(f\"Saved extracted text to {out_txt}; raw_text length = {len(raw_text)} characters.\")\n",
    "    return raw_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4142510-6931-40ce-9c4b-3ab17fa250bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example execution\n",
    "url = \"https://mobilityretail.sbi/sbustaticweb/mobile/faq_features.html\"\n",
    "raw_text = extract_text_from_url_with_smol_and_save(url)\n",
    "print(\"--- preview (first 800 chars) ---\")\n",
    "print(raw_text[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a96b22b-75b0-4f6c-9df1-85a8a689cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = RAGDocument(\n",
    "document_id=\"raw_text_002\",\n",
    "content=raw_text,\n",
    "mime_type=\"text/plain\",\n",
    "metadata={\"source\": \"SBI_url\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed8206d1-b26d-42a3-a1ad-847d9859cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text ingested successfully\n"
     ]
    }
   ],
   "source": [
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=[document],\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=100,\n",
    ")\n",
    "print(\"Raw text ingested successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "091683e5-6071-48a1-94fb-e0a69d94f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"In YONO Lite SBI , How can customer do quick transfer using QR code?\"\n",
    "\n",
    "ground_truth= \"\"\"A facility available in post login section of Yono Lite SBI application, whereby Beneficiary (any SBI customer) can create a QR code. by entering details (Name, A/c Number, IFS Code etc.). The generated QR code can be shared (by the beneficiary) with any SBI customer to send money by scanning the QR code using Yono Lite SBI app:\n",
    "\n",
    "The remitter logs into Yono Lite SBI application\n",
    "Select Quick Transfer and Send Money using QR Code.\n",
    "Scans the QR code on the beneficiary mobile device using his/her smart phone camera.\n",
    "Or reads the QR code received through various modes from which it can be shared.\n",
    "The application decodes the QR code and auto populates the beneficiary details and provides information like Name, A/c No, IFSC, etc.\n",
    "\n",
    "\n",
    "The remitter enters the “Amount” and “Remarks” and proceeds for payment. No prior registration of the beneficiary is required!\n",
    "\n",
    "\n",
    "Note : Creation and Scanning of QR Code is currently available on Android devices only.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5acd2-24c8-4deb-96b2-a6b44fdfdf04",
   "metadata": {},
   "source": [
    "## LLM Response (With RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1051f33d-2f19-43d8-b409-b3f5e9ad279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In YONO Lite SBI, customers can perform a quick transfer using QR code by following these steps:\n",
      "\n",
      "1. Log into the Yono Lite SBI application using your internet banking credentials.\n",
      "2. Navigate to the 'Quick Transfer' option.\n",
      "3. Select 'Send Money using Account Details'.\n",
      "4. Enter the beneficiary's details manually, including Name, A/c Number, IFSC Code, etc.\n",
      "5. Confirm the details and complete the transaction.\n",
      "\n",
      "The remitter, upon receiving the QR code from the beneficiary, can then:\n",
      "\n",
      "1. Log into the Yono Lite SBI application.\n",
      "2. Select 'Quick Transfer' and 'Send Money using QR Code'.\n",
      "3. Scan the QR code on the beneficiary's mobile device using their smartphone camera.\n",
      "4. The application will decode the QR code and auto-populate the beneficiary's details.\n",
      "\n",
      "Please note that this feature is currently available only on Android devices.\n"
     ]
    }
   ],
   "source": [
    "# 1) Run retrieval explicitly (same tool used by the agent)\n",
    "rag_result = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=query,\n",
    "    query_config={\n",
    "        \"chunk_size_in_tokens\": 512,\n",
    "        \"chunk_overlap_in_tokens\": 100,\n",
    "        \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2) Extract the retrieved text chunks (simple join)\n",
    "retrieved_texts = []\n",
    "for item in getattr(rag_result, \"content\", []) or []:\n",
    "    # item may be dict-like or object; handle both\n",
    "    txt = item.get(\"text\") if isinstance(item, dict) else getattr(item, \"text\", None)\n",
    "    if not txt:\n",
    "        continue\n",
    "    # remove our template labels to keep only chunk text\n",
    "    import re\n",
    "    cleaned = re.sub(r\"^Result\\s*\\d+\\s*\", \"\", txt, flags=re.I)\n",
    "    cleaned = re.sub(r\"Content:\\s*\", \"\", cleaned, flags=re.I)\n",
    "    cleaned = re.sub(r\"\\nMetadata:.*$\", \"\", cleaned, flags=re.S|re.I)\n",
    "    cleaned = cleaned.strip()\n",
    "    if cleaned:\n",
    "        retrieved_texts.append(cleaned)\n",
    "\n",
    "context = \"\\n\\n\".join(retrieved_texts).strip()\n",
    "\n",
    "# 3) Pick an LLM model (first available LLM)\n",
    "models = client.models.list()\n",
    "model_id = next((m.identifier for m in models if getattr(m, \"model_type\", None) == \"llm\"), None)\n",
    "if model_id is None:\n",
    "    raise RuntimeError(\"No LLM model found in client.models.list()\")\n",
    "\n",
    "# 4) Call the inference API (chat completion) with the retrieved context + user question\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. USE ONLY the provided CONTEXT to answer.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "]\n",
    "\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "# 5) Extract final answer text from the inference response and print it\n",
    "final_text = None\n",
    "if hasattr(resp, \"completion_message\") and getattr(resp, \"completion_message\") is not None:\n",
    "    final_text = getattr(resp.completion_message, \"content\", None) or getattr(resp.completion_message, \"text\", None)\n",
    "if not final_text and hasattr(resp, \"choices\") and resp.choices:\n",
    "    c0 = resp.choices[0]\n",
    "    if hasattr(c0, \"message\"):\n",
    "        final_text = getattr(c0.message, \"content\", None) or getattr(c0.message, \"text\", None)\n",
    "if not final_text:\n",
    "    # fallback to string representation\n",
    "    final_text = str(resp)\n",
    "\n",
    "print(final_text.strip())\n",
    "\n",
    "rag_answer= final_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67015231-a694-4e69-a014-6938eb0f253d",
   "metadata": {},
   "source": [
    "## LLM Response (Without RAG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65374d81-b3d8-4cb3-899b-efc18d8054d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base LLM Answer:\n",
      " 1. Open YONO Lite SBI app and select the \"Transfer\" option.\n",
      "2. Choose \"Quick Transfer\" or \"QR Code Transfer.\"\n",
      "3. Enter the recipient's account number and the amount to transfer.\n",
      "4. Scan the recipient's QR code using the app's camera or manually enter the account number.\n",
      "5. Review the details and confirm the transfer.\n",
      "6. The recipient will receive a notification with the transfer details.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\\nQUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "]\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "# Extract final answer text\n",
    "answer = None\n",
    "if hasattr(resp, \"completion_message\") and resp.completion_message:\n",
    "    answer = resp.completion_message.content\n",
    "elif hasattr(resp, \"choices\") and resp.choices:\n",
    "    answer = resp.choices[0].message.content\n",
    "else:\n",
    "    answer = str(resp)\n",
    "\n",
    "print(\"Base LLM Answer:\\n\", answer)\n",
    "\n",
    "base_answer=answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47f512-3c5f-47e1-95c3-8949e01c140e",
   "metadata": {},
   "source": [
    "## Comparison between Actual Answer , RAG Generated Answer, Without RAG generated answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c45f72fc-0e7e-4201-b2f8-b741d37ef9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ground Truth ===\n",
      "\n",
      "A facility available in post login section of Yono Lite SBI application, whereby Beneficiary (any SBI customer) can create a QR code. by entering details (Name, A/c Number, IFS Code etc.). The generated QR code can be shared (by the beneficiary) with any SBI customer to send money by scanning the QR code using Yono Lite SBI app:\n",
      "\n",
      "The remitter logs into Yono Lite SBI application\n",
      "Select Quick Transfer and Send Money using QR Code.\n",
      "Scans the QR code on the beneficiary mobile device using his/her smart phone camera.\n",
      "Or reads the QR code received through various modes from which it can be shared.\n",
      "The application decodes the QR code and auto populates the beneficiary details and provides information like Name, A/c No, IFSC, etc.\n",
      "\n",
      "\n",
      "The remitter enters the “Amount” and “Remarks” and proceeds for payment. No prior registration of the beneficiary is required!\n",
      "\n",
      "\n",
      "Note : Creation and Scanning of QR Code is currently available on Android devices only.\n",
      "\n",
      "=== RAG Generated Answer ===\n",
      "\n",
      "In YONO Lite SBI, customers can perform a quick transfer using QR code by following these steps:\n",
      "\n",
      "1. Log into the Yono Lite SBI application using your internet banking credentials.\n",
      "2. Navigate to the 'Quick Transfer' option.\n",
      "3. Select 'Send Money using Account Details'.\n",
      "4. Enter the beneficiary's details manually, including Name, A/c Number, IFSC Code, etc.\n",
      "5. Confirm the details and complete the transaction.\n",
      "\n",
      "The remitter, upon receiving the QR code from the beneficiary, can then:\n",
      "\n",
      "1. Log into the Yono Lite SBI application.\n",
      "2. Select 'Quick Transfer' and 'Send Money using QR Code'.\n",
      "3. Scan the QR code on the beneficiary's mobile device using their smartphone camera.\n",
      "4. The application will decode the QR code and auto-populate the beneficiary's details.\n",
      "\n",
      "Please note that this feature is currently available only on Android devices.\n",
      "\n",
      "=== Base LLM Answer (without RAG) ===\n",
      "\n",
      "1. Open YONO Lite SBI app and select the \"Transfer\" option.\n",
      "2. Choose \"Quick Transfer\" or \"QR Code Transfer.\"\n",
      "3. Enter the recipient's account number and the amount to transfer.\n",
      "4. Scan the recipient's QR code using the app's camera or manually enter the account number.\n",
      "5. Review the details and confirm the transfer.\n",
      "6. The recipient will receive a notification with the transfer details.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Ground Truth ===\\n\")\n",
    "print(ground_truth)\n",
    "\n",
    "print(\"\\n=== RAG Generated Answer ===\\n\")\n",
    "print(rag_answer)\n",
    "\n",
    "print(\"\\n=== Base LLM Answer (without RAG) ===\\n\")\n",
    "print(base_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20e038b4-089e-47fa-bc35-ca4d47ca6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing ....\n",
    "#### Upload URL / pdf file , query & ground truth (actual answer)\n",
    "\n",
    "# Master parser: handles URL (HTML or remote PDF) and local PDF.\n",
    "# Requirements: pip install requests beautifulsoup4 pdfplumber\n",
    "\n",
    "import os, io, time, requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "\n",
    "# CONFIG: set your docling endpoint or None to skip docling attempts\n",
    "SMOLDOCLING_ENDPOINT = \"https://docling-dsdemo.apps.cluster-h97qh.h97qh.sandbox1475.opentlc.com/v1\"\n",
    "# Optional: set to False if your environment blocks SSL verification (dev only)\n",
    "VERIFY_SSL = True\n",
    "\n",
    "def _try_docling_parse_url(url: str, endpoint: str = SMOLDOCLING_ENDPOINT, timeout: int = 90) -> str:\n",
    "    if not endpoint:\n",
    "        return \"\"\n",
    "    for path in (\"/parse\", \"/predict\", \"\"):\n",
    "        try:\n",
    "            resp = requests.post(endpoint.rstrip(\"/\") + path, json={\"url\": url}, timeout=timeout, verify=VERIFY_SSL)\n",
    "            resp.raise_for_status()\n",
    "            j = resp.json()\n",
    "            text = j.get(\"text\") or j.get(\"content\") or j.get(\"extracted_text\") or j.get(\"generated_text\") or \"\"\n",
    "            if isinstance(text, list):\n",
    "                text = \"\\n\\n\".join(text)\n",
    "            if text and text.strip():\n",
    "                return text\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def _try_docling_upload_pdf_bytes(filename: str, pdf_bytes: bytes, endpoint: str = SMOLDOCLING_ENDPOINT, timeout: int = 180) -> str:\n",
    "    if not endpoint:\n",
    "        return \"\"\n",
    "    files = {\"file\": (os.path.basename(filename), pdf_bytes, \"application/pdf\")}\n",
    "    for path in (\"/parse_pdf\", \"/parse\", \"/predict\", \"\"):\n",
    "        try:\n",
    "            resp = requests.post(endpoint.rstrip(\"/\") + path, files=files, timeout=timeout, verify=VERIFY_SSL)\n",
    "            resp.raise_for_status()\n",
    "            j = resp.json()\n",
    "            text = j.get(\"text\") or j.get(\"content\") or j.get(\"extracted_text\") or j.get(\"generated_text\") or \"\"\n",
    "            if isinstance(text, list):\n",
    "                text = \"\\n\\n\".join(text)\n",
    "            if text and text.strip():\n",
    "                return text\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def _pdfplumber_extract(pdf_bytes: bytes) -> str:\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "            pages = []\n",
    "            for i, p in enumerate(pdf.pages):\n",
    "                pages.append(f\"[page:{i+1}]\\n\" + (p.extract_text() or \"\"))\n",
    "        return \"\\n\\n\".join(pages)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _beautifulsoup_extract(html_text: str) -> str:\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        for s in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            s.decompose()\n",
    "        lines = [ln.strip() for ln in soup.get_text(separator=\"\\n\").splitlines() if ln.strip()]\n",
    "        return \"\\n\\n\".join(lines)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def parse_input_doc(input_doc: str, smol_endpoint: str = SMOLDOCLING_ENDPOINT, out_txt: str = \"raw_text.txt\") -> str:\n",
    "    \"\"\"\n",
    "    Parse an input document (URL or local PDF path).\n",
    "    Returns extracted text (string) and saves to out_txt. Also sets global raw_text.\n",
    "    \"\"\"\n",
    "    extracted = \"\"\n",
    "    is_url = isinstance(input_doc, str) and input_doc.lower().startswith((\"http://\", \"https://\"))\n",
    "    is_local_file = isinstance(input_doc, str) and os.path.exists(input_doc) and input_doc.lower().endswith(\".pdf\")\n",
    "\n",
    "    # 1) If it's a URL: attempt Docling remote parse first (works for HTML or remote PDFs)\n",
    "    if is_url:\n",
    "        print(f\"[parse] Input is a URL: {input_doc}\")\n",
    "\n",
    "        # Try Docling remote parse (best-first)\n",
    "        if smol_endpoint:\n",
    "            try:\n",
    "                extracted = _try_docling_parse_url(input_doc, endpoint=smol_endpoint)\n",
    "                if extracted:\n",
    "                    print(\"[parse] extracted via SmolDocling (remote URL).\")\n",
    "            except Exception:\n",
    "                extracted = \"\"\n",
    "\n",
    "        # If docling returned nothing -> try fetching HTML then BeautifulSoup\n",
    "        if not extracted:\n",
    "            # If the URL looks like a PDF (endswith .pdf or content-type), try remote PDF docling/upload fallback\n",
    "            if input_doc.lower().endswith(\".pdf\"):\n",
    "                print(\"[parse] URL looks like a PDF; attempting remote PDF Docling or fallback.\")\n",
    "                try:\n",
    "                    # try docling again with upload-style (some endpoints accept remote pdf via url param; we try parse_url first above)\n",
    "                    extracted = _try_docling_parse_url(input_doc, endpoint=smol_endpoint)\n",
    "                except Exception:\n",
    "                    extracted = \"\"\n",
    "\n",
    "            # If still nothing, try plain HTTP fetch + BeautifulSoup\n",
    "            if not extracted:\n",
    "                try:\n",
    "                    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "                    r = requests.get(input_doc, headers=headers, timeout=60, verify=VERIFY_SSL)\n",
    "                    r.raise_for_status()\n",
    "                    html = r.text or \"\"\n",
    "                    extracted = _beautifulsoup_extract(html)\n",
    "                    if extracted:\n",
    "                        print(\"[parse] extracted via BeautifulSoup fallback (URL).\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[parse] BeautifulSoup/http fallback failed for URL: {e}\")\n",
    "                    extracted = \"\"\n",
    "\n",
    "    # 2) If it's a local PDF file: try upload to Docling first, then pdfplumber fallback\n",
    "    elif is_local_file:\n",
    "        print(f\"[parse] Input is a local PDF: {input_doc}\")\n",
    "        try:\n",
    "            with open(input_doc, \"rb\") as f:\n",
    "                pdf_bytes = f.read()\n",
    "        except Exception as e:\n",
    "            print(\"[parse] Failed to read local PDF:\", e)\n",
    "            pdf_bytes = None\n",
    "\n",
    "        if pdf_bytes:\n",
    "            # try docling upload parse\n",
    "            if smol_endpoint:\n",
    "                try:\n",
    "                    extracted = _try_docling_upload_pdf_bytes(input_doc, pdf_bytes, endpoint=smol_endpoint)\n",
    "                    if extracted:\n",
    "                        print(\"[parse] extracted via SmolDocling (local PDF upload).\")\n",
    "                except Exception:\n",
    "                    extracted = \"\"\n",
    "\n",
    "            # fallback to pdfplumber\n",
    "            if not extracted:\n",
    "                try:\n",
    "                    extracted = _pdfplumber_extract(pdf_bytes)\n",
    "                    if extracted:\n",
    "                        print(\"[parse] extracted via pdfplumber fallback (local PDF).\")\n",
    "                except Exception as e:\n",
    "                    print(\"[parse] pdfplumber fallback failed:\", e)\n",
    "                    extracted = \"\"\n",
    "\n",
    "    # 3) If it is a local non-pdf path or unknown: try to read as text\n",
    "    else:\n",
    "        # Might be a raw text input; return it as-is\n",
    "        if os.path.exists(input_doc):\n",
    "            try:\n",
    "                with open(input_doc, \"r\", encoding=\"utf-8\") as f:\n",
    "                    extracted = f.read()\n",
    "                print(\"[parse] Input is a local file (non-PDF), read as text.\")\n",
    "            except Exception as e:\n",
    "                print(\"[parse] Failed to read local file:\", e)\n",
    "                extracted = \"\"\n",
    "        else:\n",
    "            # treat as inline text\n",
    "            extracted = str(input_doc)\n",
    "            print(\"[parse] Input treated as inline text.\")\n",
    "\n",
    "    # 4) persist and return\n",
    "    try:\n",
    "        with open(out_txt, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            out_f.write(extracted or \"\")\n",
    "    except Exception as e:\n",
    "        print(\"[parse] Failed to write output file:\", e)\n",
    "\n",
    "    global raw_text\n",
    "    raw_text = extracted or \"\"\n",
    "    print(f\"[parse] Saved text to {out_txt}; length = {len(raw_text)} characters.\")\n",
    "    return raw_text\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "# local PDF: parse_input_doc(\"sample.pdf\")\n",
    "# remote PDF: parse_input_doc(\"https://example.com/some.pdf\")\n",
    "# web page: parse_input_doc(\"https://www.onlinesbi.sbi/sbf_retail.html\")\n",
    "# inline text: parse_input_doc(\"This is a short note.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b83b2e7-7a7f-48cd-849b-285ea4fa4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[parse] Input is a URL: https://www.onlinesbi.sbi/sbf_retail.html\n",
      "[parse] extracted via BeautifulSoup fallback (URL).\n",
      "[parse] Saved text to raw_text.txt; length = 222 characters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This question is for testing whether you are a human visitor and to prevent automated spam submission.\\n\\nAudio is not supported in your browser.\\n\\nWhat code is in the image?\\n\\nsubmit\\n\\nYour support ID is:  3273872632185551290.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text= parse_input_doc(\"https://www.onlinesbi.sbi/sbf_retail.html\")\n",
    "raw_text[0:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95b494-2d41-4a10-90f8-b123f433593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = RAGDocument(\n",
    "document_id=\"raw_text_10\",\n",
    "content=raw_text,\n",
    "mime_type=\"text/plain\",\n",
    "metadata={\"source\": \"SBI_url\"},\n",
    ")\n",
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=[document],\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=100,\n",
    ")\n",
    "print(\"Raw text ingested successfully\")\n",
    "\n",
    "\n",
    "# 1) Run retrieval explicitly (same tool used by the agent)\n",
    "rag_result = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=query,\n",
    "    query_config={\n",
    "        \"chunk_size_in_tokens\": 512,\n",
    "        \"chunk_overlap_in_tokens\": 100,\n",
    "        \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2) Extract the retrieved text chunks (simple join)\n",
    "retrieved_texts = []\n",
    "for item in getattr(rag_result, \"content\", []) or []:\n",
    "    # item may be dict-like or object; handle both\n",
    "    txt = item.get(\"text\") if isinstance(item, dict) else getattr(item, \"text\", None)\n",
    "    if not txt:\n",
    "        continue\n",
    "    # remove our template labels to keep only chunk text\n",
    "    import re\n",
    "    cleaned = re.sub(r\"^Result\\s*\\d+\\s*\", \"\", txt, flags=re.I)\n",
    "    cleaned = re.sub(r\"Content:\\s*\", \"\", cleaned, flags=re.I)\n",
    "    cleaned = re.sub(r\"\\nMetadata:.*$\", \"\", cleaned, flags=re.S|re.I)\n",
    "    cleaned = cleaned.strip()\n",
    "    if cleaned:\n",
    "        retrieved_texts.append(cleaned)\n",
    "\n",
    "context = \"\\n\\n\".join(retrieved_texts).strip()\n",
    "\n",
    "# 3) Pick an LLM model (first available LLM)\n",
    "models = client.models.list()\n",
    "model_id = next((m.identifier for m in models if getattr(m, \"model_type\", None) == \"llm\"), None)\n",
    "if model_id is None:\n",
    "    raise RuntimeError(\"No LLM model found in client.models.list()\")\n",
    "\n",
    "# 4) Call the inference API (chat completion) with the retrieved context + user question\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. USE ONLY the provided CONTEXT to answer.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "]\n",
    "\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "# 5) Extract final answer text from the inference response and print it\n",
    "final_text = None\n",
    "if hasattr(resp, \"completion_message\") and getattr(resp, \"completion_message\") is not None:\n",
    "    final_text = getattr(resp.completion_message, \"content\", None) or getattr(resp.completion_message, \"text\", None)\n",
    "if not final_text and hasattr(resp, \"choices\") and resp.choices:\n",
    "    c0 = resp.choices[0]\n",
    "    if hasattr(c0, \"message\"):\n",
    "        final_text = getattr(c0.message, \"content\", None) or getattr(c0.message, \"text\", None)\n",
    "if not final_text:\n",
    "    # fallback to string representation\n",
    "    final_text = str(resp)\n",
    "\n",
    "print(final_text.strip())\n",
    "\n",
    "rag_answer= final_text.strip()\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\\nQUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "]\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "# Extract final answer text\n",
    "answer = None\n",
    "if hasattr(resp, \"completion_message\") and resp.completion_message:\n",
    "    answer = resp.completion_message.content\n",
    "elif hasattr(resp, \"choices\") and resp.choices:\n",
    "    answer = resp.choices[0].message.content\n",
    "else:\n",
    "    answer = str(resp)\n",
    "\n",
    "print(\"Base LLM Answer:\\n\", answer)\n",
    "\n",
    "base_answer=answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ceb86f-ef2e-47d8-939b-82cfb2770ea4",
   "metadata": {},
   "source": [
    "# Testing (In case if you want to test the code go to below cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79c68035-b06b-4f64-bcee-d79a5b9c435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single cell: two main functions - ingest_document(...) and answer_query_with_eval(...)\n",
    "# Requirements (install if missing): pip install requests beautifulsoup4 pdfplumber sklearn llama-stack-client\n",
    "\n",
    "import os, io, time, uuid, re, requests\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# ---------- CONFIG (edit for your environment) ----------\n",
    "SMOLDOCLING_ENDPOINT = globals().get(\"SMOLDOCLING_ENDPOINT\", None)  # set to your docling URL or None\n",
    "VECTOR_DB_ID = globals().get(\"vector_db_id\", globals().get(\"VECTOR_DB_ID\", None))\n",
    "CLIENT = globals().get(\"client\", None)  # must be LlamaStackClient already instantiated\n",
    "VERIFY_SSL = True  # set False only in dev where TLS issues exist\n",
    "# Ingestion/retrieval defaults tuned for good behavior:\n",
    "INGEST_CHUNK_SIZE = 512\n",
    "INGEST_CHUNK_OVERLAP = 50\n",
    "RETRIEVE_CHUNK_SIZE = 512\n",
    "RETRIEVE_CHUNK_OVERLAP = 50\n",
    "RETRIEVE_TOP_K = 6\n",
    "INGEST_BATCH_DELAY = 0.5\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# ------------------ Helpers ------------------\n",
    "def _try_docling_parse_url(url: str, endpoint: Optional[str], timeout: int = 90) -> str:\n",
    "    if not endpoint:\n",
    "        return \"\"\n",
    "    for path in (\"/parse\", \"/predict\", \"\"):\n",
    "        try:\n",
    "            resp = requests.post(endpoint.rstrip(\"/\") + path, json={\"url\": url}, timeout=timeout, verify=VERIFY_SSL)\n",
    "            resp.raise_for_status()\n",
    "            j = resp.json()\n",
    "            text = j.get(\"text\") or j.get(\"content\") or j.get(\"extracted_text\") or j.get(\"generated_text\") or \"\"\n",
    "            if isinstance(text, list):\n",
    "                text = \"\\n\\n\".join(text)\n",
    "            if text and text.strip():\n",
    "                return text\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def _try_docling_upload_pdf_bytes(filename: str, pdf_bytes: bytes, endpoint: Optional[str], timeout: int = 180) -> str:\n",
    "    if not endpoint:\n",
    "        return \"\"\n",
    "    files = {\"file\": (os.path.basename(filename), pdf_bytes, \"application/pdf\")}\n",
    "    for path in (\"/parse_pdf\", \"/parse\", \"/predict\", \"\"):\n",
    "        try:\n",
    "            resp = requests.post(endpoint.rstrip(\"/\") + path, files=files, timeout=timeout, verify=VERIFY_SSL)\n",
    "            resp.raise_for_status()\n",
    "            j = resp.json()\n",
    "            text = j.get(\"text\") or j.get(\"content\") or j.get(\"extracted_text\") or j.get(\"generated_text\") or \"\"\n",
    "            if isinstance(text, list):\n",
    "                text = \"\\n\\n\".join(text)\n",
    "            if text and text.strip():\n",
    "                return text\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def _pdfplumber_extract_from_bytes(pdf_bytes: bytes) -> str:\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "            pages = []\n",
    "            for i, p in enumerate(pdf.pages):\n",
    "                pages.append(f\"[page:{i+1}]\\n\" + (p.extract_text() or \"\"))\n",
    "        return \"\\n\\n\".join(pages)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _beautifulsoup_extract(html_text: str) -> str:\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        for s in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            s.decompose()\n",
    "        lines = [ln.strip() for ln in soup.get_text(separator=\"\\n\").splitlines() if ln.strip()]\n",
    "        return \"\\n\\n\".join(lines)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _tfidf_cosine(a: str, b: str) -> float:\n",
    "    try:\n",
    "        vect = TfidfVectorizer().fit([a or \"\", b or \"\"])\n",
    "        tfidf = vect.transform([a or \"\", b or \"\"])\n",
    "        sim = cosine_similarity(tfidf[0], tfidf[1])[0][0]\n",
    "        return float(sim)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def _choose_llm_model_id(client) -> str:\n",
    "    models = client.models.list()\n",
    "    model_id = next((m.identifier for m in models if getattr(m, \"model_type\", None) == \"llm\"), None)\n",
    "    if model_id is None and models:\n",
    "        model_id = models[0].identifier\n",
    "    return model_id\n",
    "\n",
    "def _clean_rag_template_text(raw_text: str) -> str:\n",
    "    # remove \"Result {i} Content: ... Metadata: ...\" wrappers used by chunk_template\n",
    "    cleaned = re.sub(r\"^Result\\s*\\d+\\s*\", \"\", raw_text, flags=re.I|re.M)\n",
    "    cleaned = re.sub(r\"Content:\\s*\", \"\", cleaned, flags=re.I)\n",
    "    cleaned = re.sub(r\"\\nMetadata:.*$\", \"\", cleaned, flags=re.S|re.I)\n",
    "    return cleaned.strip()\n",
    "\n",
    "# --------------- Main function 1: ingest_document ----------------\n",
    "def ingest_document(\n",
    "    input_doc: str,\n",
    "    client = CLIENT,\n",
    "    vector_db_id: str = VECTOR_DB_ID,\n",
    "    smol_endpoint: Optional[str] = SMOLDOCLING_ENDPOINT,\n",
    "    chunk_size_in_tokens: int = INGEST_CHUNK_SIZE,\n",
    "    chunk_overlap_in_tokens: int = INGEST_CHUNK_OVERLAP,\n",
    "    out_txt: str = \"raw_text.txt\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse input_doc (URL / local PDF / inline text), create a RAGDocument with metadata,\n",
    "    insert into vector DB via client.tool_runtime.rag_tool.insert(), and return {document_id, insert_response, source_type}.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"client (LlamaStackClient) is not provided. Set 'client' variable.\")\n",
    "\n",
    "    # detect type\n",
    "    is_url = isinstance(input_doc, str) and input_doc.lower().startswith((\"http://\", \"https://\"))\n",
    "    is_local_pdf = isinstance(input_doc, str) and os.path.exists(input_doc) and input_doc.lower().endswith(\".pdf\")\n",
    "\n",
    "    # parse text (Docling-first)\n",
    "    extracted = \"\"\n",
    "    parser_used = None\n",
    "\n",
    "    # URL handling (remote PDF or webpage)\n",
    "    if is_url:\n",
    "        # Remote PDF?\n",
    "        if input_doc.lower().endswith(\".pdf\"):\n",
    "            # fetch bytes then try docling upload or local extraction\n",
    "            try:\n",
    "                r = requests.get(input_doc, timeout=60, verify=VERIFY_SSL)\n",
    "                r.raise_for_status()\n",
    "                pdf_bytes = r.content\n",
    "            except Exception as e:\n",
    "                pdf_bytes = None\n",
    "            if pdf_bytes:\n",
    "                # try docling upload (remote pdf)\n",
    "                parsed = _try_docling_upload_pdf_bytes(input_doc, pdf_bytes, endpoint=smol_endpoint)\n",
    "                if parsed:\n",
    "                    extracted = parsed; parser_used = \"docling_remote_pdf\"\n",
    "                else:\n",
    "                    # fallback to pdfplumber\n",
    "                    extracted = _pdfplumber_extract_from_bytes(pdf_bytes)\n",
    "                    parser_used = \"pdfplumber_remote_pdf\" if extracted else None\n",
    "        else:\n",
    "            # try docling parse(url)\n",
    "            parsed = _try_docling_parse_url(input_doc, endpoint=smol_endpoint)\n",
    "            if parsed:\n",
    "                extracted = parsed; parser_used = \"docling_url\"\n",
    "            else:\n",
    "                # fallback to BeautifulSoup\n",
    "                try:\n",
    "                    r = requests.get(input_doc, timeout=60, verify=VERIFY_SSL)\n",
    "                    r.raise_for_status()\n",
    "                    extracted = _beautifulsoup_extract(r.text)\n",
    "                    parser_used = \"bs4_url\" if extracted else None\n",
    "                except Exception:\n",
    "                    extracted = \"\"\n",
    "\n",
    "    # local PDF handling\n",
    "    elif is_local_pdf:\n",
    "        try:\n",
    "            with open(input_doc, \"rb\") as f:\n",
    "                pdf_bytes = f.read()\n",
    "        except Exception:\n",
    "            pdf_bytes = None\n",
    "        if pdf_bytes:\n",
    "            parsed = _try_docling_upload_pdf_bytes(input_doc, pdf_bytes, endpoint=smol_endpoint)\n",
    "            if parsed:\n",
    "                extracted = parsed; parser_used = \"docling_local_pdf\"\n",
    "            else:\n",
    "                extracted = _pdfplumber_extract_from_bytes(pdf_bytes)\n",
    "                parser_used = \"pdfplumber_local_pdf\" if extracted else None\n",
    "\n",
    "    # otherwise treat as inline text (or path to text)\n",
    "    else:\n",
    "        if isinstance(input_doc, str) and os.path.exists(input_doc):\n",
    "            try:\n",
    "                with open(input_doc, \"r\", encoding=\"utf-8\") as f:\n",
    "                    extracted = f.read()\n",
    "                parser_used = \"local_text_file\"\n",
    "            except Exception:\n",
    "                extracted = str(input_doc); parser_used = \"inline_text_fallback\"\n",
    "        else:\n",
    "            extracted = str(input_doc or \"\"); parser_used = \"inline_text\"\n",
    "\n",
    "    # save extracted text\n",
    "    try:\n",
    "        with open(out_txt, \"w\", encoding=\"utf-8\") as o:\n",
    "            o.write(extracted or \"\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # prepare metadata\n",
    "    document_id = f\"doc-{uuid.uuid4().hex[:8]}\"\n",
    "    if is_url:\n",
    "        source_type = \"pdf_url\" if input_doc.lower().endswith(\".pdf\") else \"url\"\n",
    "    elif is_local_pdf:\n",
    "        source_type = \"pdf_local\"\n",
    "    else:\n",
    "        source_type = \"text_inline\"\n",
    "\n",
    "    metadata = {\n",
    "        \"source\": input_doc,\n",
    "        \"source_type\": source_type,\n",
    "        \"parser\": parser_used,\n",
    "        \"document_id\": document_id,\n",
    "    }\n",
    "\n",
    "    # Create RAGDocument object (explicit text content) - server will chunk & embed\n",
    "    rag_doc = RAGDocument(\n",
    "        document_id=document_id,\n",
    "        content={\"type\": \"text\", \"text\": extracted or \"\"},\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    # Insert into vector DB\n",
    "    try:\n",
    "        resp = client.tool_runtime.rag_tool.insert(\n",
    "            documents=[rag_doc],\n",
    "            vector_db_id=vector_db_id,\n",
    "            chunk_size_in_tokens=chunk_size_in_tokens,\n",
    "        )\n",
    "        time.sleep(INGEST_BATCH_DELAY)\n",
    "    except Exception as e:\n",
    "        # bubble up but include context\n",
    "        raise RuntimeError(f\"Insert failed: {e}\")\n",
    "\n",
    "    return {\"document_id\": document_id, \"insert_response\": resp, \"source_type\": source_type, \"parser_used\": parser_used}\n",
    "\n",
    "# --------------- Main function 2: answer_query_with_eval ----------------\n",
    "def answer_query_with_eval(\n",
    "    query: str,\n",
    "    ground_truth: str,\n",
    "    document_id: str,\n",
    "    client = CLIENT,\n",
    "    vector_db_id: str = VECTOR_DB_ID,\n",
    "    retrieve_top_k: int = RETRIEVE_TOP_K,\n",
    "    retrieve_chunk_size: int = RETRIEVE_CHUNK_SIZE,\n",
    "    retrieve_chunk_overlap: int = RETRIEVE_CHUNK_OVERLAP,\n",
    "    llm_model_id: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a user query, ground_truth, and a single previously-ingested document_id (in vector DB),\n",
    "    perform:\n",
    "      - retrieval from vector DB (rag_tool.query)\n",
    "      - RAG answer (LLM with retrieved context)\n",
    "      - Base LLM answer (no retrieval)\n",
    "      - Basic eval metrics (TF-IDF cosine to ground_truth, lengths, grounding proxy)\n",
    "    Returns dict containing rag_answer, base_answer, metrics, retrieved_chunks, document_id.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"client is not provided.\")\n",
    "\n",
    "    # 1) retrieval\n",
    "    rag_result = client.tool_runtime.rag_tool.query(\n",
    "        vector_db_ids=[vector_db_id],\n",
    "        content=query,\n",
    "        query_config={\n",
    "            \"chunk_size_in_tokens\": retrieve_chunk_size,\n",
    "            \"chunk_overlap_in_tokens\": retrieve_chunk_overlap,\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "            \"top_k\": retrieve_top_k,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # extract cleaned retrieved chunks\n",
    "    retrieved_chunks = []\n",
    "    for item in getattr(rag_result, \"content\", []) or []:\n",
    "        raw_txt = item.get(\"text\") if isinstance(item, dict) else getattr(item, \"text\", None)\n",
    "        if not raw_txt:\n",
    "            continue\n",
    "        cleaned = _clean_rag_template_text(raw_txt)\n",
    "        if cleaned:\n",
    "            retrieved_chunks.append(cleaned)\n",
    "\n",
    "    # Build context for RAG LLM (label sources)\n",
    "    context_parts = []\n",
    "    for i, chunk in enumerate(retrieved_chunks[:retrieve_top_k], start=1):\n",
    "        # try to extract metadata document id from chunk object if available in raw event\n",
    "        # simpler: just mark SRC_i\n",
    "        context_parts.append(f\"[SRC_{i}]\\n{chunk}\")\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_parts).strip()\n",
    "\n",
    "    # 2) pick LLM model\n",
    "    model_id = llm_model_id or _choose_llm_model_id(client)\n",
    "    if not model_id:\n",
    "        raise RuntimeError(\"No LLM model available via client.models.list()\")\n",
    "\n",
    "    # 3) RAG answer - instruct to use only context\n",
    "    if context_text:\n",
    "        messages_rag = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. USE ONLY the provided CONTEXT to answer the question. When using context, append inline citations like [SRC_1]. If the answer is not present in the context, say 'I don't know.'\"},\n",
    "            {\"role\": \"user\", \"content\": f\"CONTEXT:\\n{context_text}\\n\\nQUESTION:\\n{query}\\n\\nAnswer concisely and include inline citations where used.\"}\n",
    "        ]\n",
    "    else:\n",
    "        messages_rag = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. No retrieved context was found.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "        ]\n",
    "\n",
    "    resp_rag = client.inference.chat_completion(messages=messages_rag, model_id=model_id)\n",
    "    # robust extraction\n",
    "    rag_answer = None\n",
    "    if hasattr(resp_rag, \"completion_message\") and getattr(resp_rag, \"completion_message\") is not None:\n",
    "        rag_answer = getattr(resp_rag.completion_message, \"content\", None) or getattr(resp_rag.completion_message, \"text\", None)\n",
    "    elif hasattr(resp_rag, \"choices\") and resp_rag.choices:\n",
    "        c0 = resp_rag.choices[0]\n",
    "        rag_answer = getattr(c0.message, \"content\", None) or getattr(c0.message, \"text\", None)\n",
    "    else:\n",
    "        rag_answer = str(resp_rag)\n",
    "    rag_answer = (rag_answer or \"\").strip()\n",
    "\n",
    "    # 4) Base LLM answer (no context)\n",
    "    messages_base = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"QUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "    ]\n",
    "    resp_base = client.inference.chat_completion(messages=messages_base, model_id=model_id)\n",
    "    base_answer = None\n",
    "    if hasattr(resp_base, \"completion_message\") and getattr(resp_base, \"completion_message\") is not None:\n",
    "        base_answer = getattr(resp_base.completion_message, \"content\", None) or getattr(resp_base.completion_message, \"text\", None)\n",
    "    elif hasattr(resp_base, \"choices\") and resp_base.choices:\n",
    "        c0 = resp_base.choices[0]\n",
    "        base_answer = getattr(c0.message, \"content\", None) or getattr(c0.message, \"text\", None)\n",
    "    else:\n",
    "        base_answer = str(resp_base)\n",
    "    base_answer = (base_answer or \"\").strip()\n",
    "\n",
    "    # 5) Evaluation metrics\n",
    "    sim_rag_gt = _tfidf_cosine(rag_answer, ground_truth)\n",
    "    sim_base_gt = _tfidf_cosine(base_answer, ground_truth)\n",
    "    # grounding proxy: fraction of sentences in rag_answer that contain a 4-word substring from any retrieved chunk\n",
    "    def grounding_fraction(answer_text: str, chunks: List[str]) -> float:\n",
    "        if not answer_text.strip() or not chunks:\n",
    "            return 0.0\n",
    "        sents = re.split(r\"[.?!]\\s+\", answer_text)\n",
    "        matched = 0\n",
    "        for sent in sents:\n",
    "            sent_low = sent.lower()\n",
    "            found = False\n",
    "            # check n-grams of length 4 from each chunk\n",
    "            for ch in chunks:\n",
    "                tokens = ch.split()\n",
    "                for i in range(max(0, len(tokens)-3)):\n",
    "                    phrase = \" \".join(tokens[i:i+4]).lower()\n",
    "                    if phrase and phrase in sent_low:\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "            if found:\n",
    "                matched += 1\n",
    "        return matched / max(1, len(sents))\n",
    "\n",
    "    grounding_score = grounding_fraction(rag_answer, retrieved_chunks)\n",
    "\n",
    "    metrics = {\n",
    "        \"sim_rag_vs_ground_truth_tfidf\": round(sim_rag_gt, 4),\n",
    "        \"sim_base_vs_ground_truth_tfidf\": round(sim_base_gt, 4),\n",
    "        \"length_rag_words\": len(rag_answer.split()),\n",
    "        \"length_base_words\": len(base_answer.split()),\n",
    "        \"retrieved_chunk_count\": len(retrieved_chunks),\n",
    "        \"grounding_fraction\": round(grounding_score, 4),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"document_id\": document_id,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"rag_answer\": rag_answer,\n",
    "        \"base_answer\": base_answer,\n",
    "        \"metrics\": metrics,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b98419-3846-4ecc-b4fd-50f7b7070518",
   "metadata": {},
   "source": [
    "### Testing for pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b18182-7bdb-4f39-8426-64a19ac4f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your pdf and test it \n",
    "\n",
    "pdf_name= \"RAG_demo_doc.pdf\"\n",
    "\n",
    "### Type in your query\n",
    "\n",
    "query=\"What are the conditions for uploading photographs?\"\n",
    "\n",
    "### Type your ground truth (Actual Answer)\n",
    "\n",
    "ground_truth =\"\"\" Photograph Image: (4.5 cm x 3.5 cm)\n",
    "• Photograph must be a recent passport style colour picture.\n",
    "• Make sure that the picture is in colour, taken against a light-coloured, preferably white,\n",
    "background.\n",
    "• Look straight at the camera with a relaxed face\n",
    "• If picture is taken on a sunny day, have the sun behind you, or place yourself in shade,\n",
    "so that you are not squinting and there are no harsh shadows\n",
    "• If you have to use flash, ensure there's no \"red-eye\"\n",
    "• If you wear glasses make sure that there are no reflections and your eyes can be\n",
    "clearly seen.\n",
    "• Caps, hats and dark glasses are not acceptable. Religious headwear is allowed but it\n",
    "must not cover your face.\n",
    "• Dimensions 200 x 230 pixels (preferred)\n",
    "• Size of file should be between 20 kb–50 kb\n",
    "• Ensure that size of the scanned image is not more than 50kb. If the size of the file is\n",
    "more than 50 kb, then adjust the settings of the scanner such as the DPI resolution,\n",
    "no. of colours etc., during the process of scanning.\n",
    "• Photo uploaded should be of appropriate size and clearly visible.\n",
    "• It is advisable that candidate retains about 8 copies of the same photograph\n",
    "which is uploaded at the time of online application as these would be needed for\n",
    "further processes of this selection process.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "387f29c2-35ca-435c-a8f1-9ef8ce2d1b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P28' is an invalid float value\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document_id: doc-fccfbdf5\n"
     ]
    }
   ],
   "source": [
    "# 1) Ingest a document (URL or local PDF or inline text)\n",
    "res_ingest = ingest_document(pdf_name, client=client, vector_db_id=vector_db_id)\n",
    "print(\"Inserted document_id:\", res_ingest[\"document_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b4c8547-ff2e-4bb4-92e4-0d20d63e8c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "#2) Ask query and evaluate\n",
    "out = answer_query_with_eval(\n",
    "    query=query,\n",
    "    ground_truth=ground_truth,\n",
    "    document_id=res_ingest[\"document_id\"],\n",
    "    client=client,\n",
    "    vector_db_id=vector_db_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a90c6ffb-3d01-46f0-a5be-0a9ff591cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sim_rag_vs_ground_truth_tfidf': 0.4943, 'sim_base_vs_ground_truth_tfidf': 0.3421, 'length_rag_words': 223, 'length_base_words': 107, 'retrieved_chunk_count': 8, 'grounding_fraction': 0.3158}\n",
      "RAG answer: To upload photographs for the application process, candidates must adhere to several conditions:\n",
      "\n",
      "1. **Size and Format:** The photograph should be a recent passport-style color picture, with a minimum size of 200 x 230 pixels (preferred) [SRC_5](5). The file type should be JPG or JPEG [SRC_6](6).\n",
      "\n",
      "2. **Background and Lighting:** The photograph should be taken against a light-colored, preferably white, background [SRC_4](4). Ensure adequate lighting, and avoid using flash if possible, to prevent \"red-eye\" [SRC_4](4).\n",
      "\n",
      "3. **Face Clarity:** The candidate's face should be clearly visible, with no harsh shadows or glasses reflections [SRC_4](4). Candidates should look straight at the webcam or mobile phone [SRC_4](4).\n",
      "\n",
      "4. **File Size:** The file size should be between 20 kb–50 kb [SRC_5](5). If the size exceeds 50 kb, adjust the scanner settings such as DPI resolution, number of colors, etc. [SRC_5](5)\n",
      "\n",
      "5. **Upload Procedure:** While filling the Online Application Form, candidates will receive links to upload their photograph, signature, left thumb impression, and handwritten declaration [SRC_5](5). After selecting the respective link, click 'Open/Upload' and choose the file from the location where it has been saved [SRC_5](5).\n",
      "\n",
      "6. **Additional Live Photograph:** Candidates are also required to capture and upload their live photograph using either a webcam or mobile phone [SRC_5](5).\n",
      "\n",
      "7. **No Acceptable Items:** Caps, hats, dark glasses, and religious headwear that covers the face are not acceptable [SRC_5](5).\n",
      "Base answer: To upload photographs, ensure they meet these conditions:\n",
      "\n",
      "1. Relevance: The images should be pertinent to the context or purpose of the upload.\n",
      "2. Quality: High-resolution, clear, and well-lit photos are preferred.\n",
      "3. Size: Files should not exceed the specified limit to avoid platform issues.\n",
      "4. Copyright: You must own the rights to the images or have permission to use them.\n",
      "5. Content: Avoid inappropriate, offensive, or copyright-infringing material.\n",
      "6. Format: Common formats like JPEG, PNG, or GIF are typically accepted.\n",
      "7. Permissions: If using images of people, ensure you have their consent for publication.\n",
      "\n",
      "Always review the specific guidelines provided by the platform for uploading photographs.\n"
     ]
    }
   ],
   "source": [
    "print(out[\"metrics\"])\n",
    "print(\"RAG answer:\", out[\"rag_answer\"])\n",
    "print(\"Base answer:\", out[\"base_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85acb00a-1fa1-490c-9ca1-ff74827836a5",
   "metadata": {},
   "source": [
    "## Testing For URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12071b58-f56c-49d7-bc4e-a4c2d331f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "### input url\n",
    "url= \"https://mobilityretail.sbi/sbustaticweb/mobile/faq_features.html\"\n",
    "\n",
    "### question\n",
    "query=\"Can I view a list of all cheques which I have stopped?\"\n",
    "\n",
    "### Actual answer\n",
    "ground_truth =\"\"\"Yes. Please follow the following steps to check the list of stopped cheques.\n",
    "\n",
    "Login to Yono Lite SBI.\n",
    "Click on 'Requests' >> 'Cheque Book' >> 'Stop/Revoke Cheque'\n",
    "Select the radio button 'View Recent'.\n",
    "Select the account number from the dropdown.\n",
    "The app will show all the cheques that you have stopped with their reference number and start cheque numbers.\n",
    "\n",
    "To see in detail, please select any of the entry and it will show the details.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8e243db-6edf-41bd-931c-8483ff16b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document_id: doc-0bf1edc5\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Example usage ----------------\n",
    "# 1) Ingest a document (URL or local PDF or inline text)\n",
    "res_ingest = ingest_document(url, client=client, vector_db_id=vector_db_id)\n",
    "print(\"Inserted document_id:\", res_ingest[\"document_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f50142c-edda-404f-aff2-3d44f517543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "#2) Ask query and evaluate\n",
    "out = answer_query_with_eval(\n",
    "    query=query,\n",
    "    ground_truth=ground_truth,\n",
    "    document_id=res_ingest[\"document_id\"],\n",
    "    client=client,\n",
    "    vector_db_id=vector_db_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14fbc5dc-dc6b-47d3-9b0b-3da2bd242ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sim_rag_vs_ground_truth_tfidf': 0.6661, 'sim_base_vs_ground_truth_tfidf': 0.178, 'length_rag_words': 93, 'length_base_words': 37, 'retrieved_chunk_count': 8, 'grounding_fraction': 0.4286}\n",
      "RAG answer: Yes, you can view a list of all cheques that you have stopped. To do this, follow these steps:\n",
      "\n",
      "1. Login to Yono Lite SBI [SRC_4].\n",
      "2. Click on 'Requests' >> 'Cheque Book' >> 'Stop/Revoke Cheque' [SRC_4].\n",
      "3. Select the radio button 'View Recent' [SRC_6].\n",
      "4. Select the account number from the dropdown [SRC_6].\n",
      "5. The app will display all the cheques you have stopped with their reference number and start cheque numbers [SRC_6].\n",
      "6. To see details of any specific cheque, select it [SRC_6].\n",
      "\n",
      "This information is provided in [SRC_4] and [SRC_6].\n",
      "Base answer: Yes, you can typically view a list of stopped cheques in your bank account through online banking, mobile app, or by contacting customer service. The process may vary by bank, so refer to your bank's specific instructions.\n"
     ]
    }
   ],
   "source": [
    "print(out[\"metrics\"])\n",
    "print(\"RAG answer:\", out[\"rag_answer\"])\n",
    "print(\"Base answer:\", out[\"base_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df021b83-4ca7-47fe-b74e-d5398f5f9c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378827d-c28d-4c4b-8c7a-8dd037c4242f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f787f0-97ec-4b3d-a1ee-39307d1c032b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8e261-cd32-4233-888b-5ddefba58261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238350be-955b-422d-a6d5-6b5f055cf378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c6a39-ca97-484f-a69d-37ff73232171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ce835-1d57-4377-8300-1ebb3341643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rough Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3dae52-87c2-4229-b771-8f23f834ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import RAGDocument, LlamaStackClient\n",
    "deployment_endpoint= \"http://lsd-llama-milvus-service:8321\"\n",
    "client = LlamaStackClient(base_url=deployment_endpoint)\n",
    "models= client.models.list()\n",
    "print(client.models.list())\n",
    "model_id = next(m.identifier for m in models if m.model_type == \"llm\")\n",
    "embedding_model = next(m for m in models if m.model_type == \"embedding\")\n",
    "embedding_model_id = embedding_model.identifier\n",
    "embedding_dimension = embedding_model.metadata[\"embedding_dimension\"]\n",
    "print(client.vector_dbs.list()) # lists available connectors\n",
    "\n",
    "vector_db_id = \"my_milvus_db\"\n",
    "provider_id  = \"milvus\"\n",
    "\n",
    "# Cell 1: dependencies & config (edit endpoint and file path)\n",
    "import io\n",
    "import requests\n",
    "import pdfplumber\n",
    "\n",
    "# EDIT: put your SmolDocling base URL here (as you provided earlier)\n",
    "SMOLDOCLING_ENDPOINT = \"https://docling-dsdemo.apps.cluster-h97qh.h97qh.sandbox1475.opentlc.com/v1\"\n",
    "\n",
    "# Path to the PDF you want to extract\n",
    "pdf_path = \"RAG_demo_doc.pdf\"   # <- change to your pdf file path\n",
    "\n",
    "# Cell 2: extract text via SmolDocling (with pdfplumber fallback), save to `raw_text`\n",
    "def extract_text_with_smol_and_save(pdf_path: str, smol_endpoint: str = SMOLDOCLING_ENDPOINT, out_txt: str = \"raw_text.txt\") -> str:\n",
    "    \"\"\"\n",
    "    1) Try to POST the PDF to SmolDocling's parse endpoint (assumes POST <endpoint>/parse_pdf accepts multipart/form-data file).\n",
    "    2) If that fails or returns no text, fallback to local extraction using pdfplumber.\n",
    "    3) Save the final text to out_txt and also return it (and set raw_text).\n",
    "    \"\"\"\n",
    "    # read bytes\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "\n",
    "    # 1) Try SmolDocling parse route\n",
    "    extracted = \"\"\n",
    "    try:\n",
    "        files = {\"file\": (pdf_path, pdf_bytes, \"application/pdf\")}\n",
    "        # attempt parse route first\n",
    "        resp = requests.post(smol_endpoint.rstrip(\"/\") + \"/parse_pdf\", files=files, timeout=300)\n",
    "        resp.raise_for_status()\n",
    "        j = resp.json()\n",
    "        extracted = j.get(\"text\") or j.get(\"content\") or \"\"\n",
    "        if extracted and extracted.strip():\n",
    "            print(\"Extracted text from SmolDocling /parse_pdf\")\n",
    "    except Exception as e_parse:\n",
    "        # try a generic predict route if parse_pdf doesn't exist\n",
    "        try:\n",
    "            payload = {\"file_name\": pdf_path}\n",
    "            # some deployments might accept multipart on /predict or different shapes - try /predict with same files\n",
    "            resp2 = requests.post(smol_endpoint.rstrip(\"/\") + \"/predict\", files=files, timeout=300)\n",
    "            resp2.raise_for_status()\n",
    "            j2 = resp2.json()\n",
    "            extracted = j2.get(\"text\") or j2.get(\"content\") or j2.get(\"generated_text\") or \"\"\n",
    "            if extracted and extracted.strip():\n",
    "                print(\"Extracted text from SmolDocling /predict\")\n",
    "        except Exception as e_predict:\n",
    "            print(\"SmolDocling endpoints failed (parse/predict). Falling back to local extraction.\")\n",
    "            # will fallback below\n",
    "\n",
    "    # 2) Fallback to pdfplumber if Smol didn't return text\n",
    "    if not extracted or not extracted.strip():\n",
    "        try:\n",
    "            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "                pages = []\n",
    "                for i, p in enumerate(pdf.pages):\n",
    "                    pages.append(f\"[page:{i+1}]\\n\" + (p.extract_text() or \"\"))\n",
    "            extracted = \"\\n\\n\".join(pages)\n",
    "            print(\"Extracted text using pdfplumber fallback.\")\n",
    "        except Exception as e_local:\n",
    "            # if even fallback fails, keep an empty string and raise/log\n",
    "            print(\"Local pdfplumber extraction also failed:\", e_local)\n",
    "            extracted = \"\"\n",
    "\n",
    "    # 3) Save to file and return\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(extracted or \"\")\n",
    "\n",
    "    # put into a global variable raw_text for convenience in notebook\n",
    "    global raw_text\n",
    "    raw_text = extracted or \"\"\n",
    "    print(f\"Saved extracted text to {out_txt}; raw_text length = {len(raw_text)} characters.\")\n",
    "    return raw_text\n",
    "\n",
    "# Execute extraction (will set raw_text)\n",
    "raw_text = extract_text_with_smol_and_save(pdf_path)\n",
    "# quick preview\n",
    "print(\"--- preview (first 800 chars) ---\")\n",
    "print(raw_text[:800])\n",
    "\n",
    "\n",
    "### Do this step only once \n",
    "\n",
    "\n",
    "# _ = client.vector_dbs.register(\n",
    "# vector_db_id=vector_db_id,\n",
    "# embedding_model=embedding_model_id,\n",
    "# embedding_dimension=embedding_dimension,\n",
    "# provider_id=provider_id,\n",
    "# )\n",
    "# print(f\"Registered vector DB: {vector_db_id}\")\n",
    "\n",
    "\n",
    "document = RAGDocument(\n",
    "document_id=\"raw_text_001\",\n",
    "content=raw_text,\n",
    "mime_type=\"text/plain\",\n",
    "metadata={\"source\": \"SBI_Doc\"},\n",
    ")\n",
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=[document],\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=100,\n",
    ")\n",
    "print(\"Raw text ingested successfully\")\n",
    "\n",
    "# Example RAG query for one-off lookups\n",
    "query = \"What is condition for uploading photographs ?\"\n",
    "\n",
    "# 1) Run retrieval explicitly (same tool used by the agent)\n",
    "rag_result = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=query,\n",
    "    query_config={\n",
    "        \"chunk_size_in_tokens\": 512,\n",
    "        \"chunk_overlap_in_tokens\": 0,\n",
    "        \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2) Extract the retrieved text chunks (simple join)\n",
    "retrieved_texts = []\n",
    "for item in getattr(rag_result, \"content\", []) or []:\n",
    "    # item may be dict-like or object; handle both\n",
    "    txt = item.get(\"text\") if isinstance(item, dict) else getattr(item, \"text\", None)\n",
    "    if not txt:\n",
    "        continue\n",
    "    # remove our template labels to keep only chunk text\n",
    "    import re\n",
    "    cleaned = re.sub(r\"^Result\\s*\\d+\\s*\", \"\", txt, flags=re.I)\n",
    "    cleaned = re.sub(r\"Content:\\s*\", \"\", cleaned, flags=re.I)\n",
    "    cleaned = re.sub(r\"\\nMetadata:.*$\", \"\", cleaned, flags=re.S|re.I)\n",
    "    cleaned = cleaned.strip()\n",
    "    if cleaned:\n",
    "        retrieved_texts.append(cleaned)\n",
    "\n",
    "context = \"\\n\\n\".join(retrieved_texts).strip()\n",
    "\n",
    "# 3) Pick an LLM model (first available LLM)\n",
    "models = client.models.list()\n",
    "model_id = next((m.identifier for m in models if getattr(m, \"model_type\", None) == \"llm\"), None)\n",
    "if model_id is None:\n",
    "    raise RuntimeError(\"No LLM model found in client.models.list()\")\n",
    "\n",
    "# 4) Call the inference API (chat completion) with the retrieved context + user question\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. USE ONLY the provided CONTEXT to answer.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\\n\\nAnswer concisely:\"}\n",
    "]\n",
    "\n",
    "resp = client.inference.chat_completion(messages=messages, model_id=model_id)\n",
    "\n",
    "# 5) Extract final answer text from the inference response and print it\n",
    "final_text = None\n",
    "if hasattr(resp, \"completion_message\") and getattr(resp, \"completion_message\") is not None:\n",
    "    final_text = getattr(resp.completion_message, \"content\", None) or getattr(resp.completion_message, \"text\", None)\n",
    "if not final_text and hasattr(resp, \"choices\") and resp.choices:\n",
    "    c0 = resp.choices[0]\n",
    "    if hasattr(c0, \"message\"):\n",
    "        final_text = getattr(c0.message, \"content\", None) or getattr(c0.message, \"text\", None)\n",
    "if not final_text:\n",
    "    # fallback to string representation\n",
    "    final_text = str(resp)\n",
    "\n",
    "print(final_text.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
